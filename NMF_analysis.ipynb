{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#mounting of directory used in Boris' implementation"
      ],
      "metadata": {
        "id": "Gcvd4omw0RJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ACG Lab/Dyakov et al 2023 (google drive version)/Cluster analysis/7013 MSPLIT')"
      ],
      "metadata": {
        "id": "QWH8d0CqOaR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318be195-7b09-418d-d274-04f8bc1dceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#install all dependencies"
      ],
      "metadata": {
        "id": "f9Kph5cr0MOK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPdUMjFdpDkY"
      },
      "outputs": [],
      "source": [
        "!pip install gprofiler-official\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from sklearn.decomposition import NMF, PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from gprofiler import GProfiler\n",
        "from collections import Counter\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import csv\n",
        "import pickle\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NMF analysis\n",
        "\n",
        "Using the SAINT results for the BirA* dataset, this code runs the NMF analysis which produces the files included in Supplementary Table 5:\n",
        "\n",
        "\n",
        "*   Prey matrix\n",
        "*   Bait matrix\n",
        "*   Clusters (gene lists)\n",
        "*   Gene set enrichment analysis results using g:Profiler\n",
        "\n",
        "These results are displayed in Figs. 2 and 3 and Extended Data Figs. 5-7\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "olvE6TXFj1hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nmf_cluster_analysis_filtering(n_components):\n",
        "    def norm_avg_ctrl(df):\n",
        "        ctrls = [i.split('|') for i in df['ctrlCounts']]\n",
        "        ### choose either of the next two lines depending on if you wanna use all controls for subtraction ###\n",
        "        ### or top 10 control counts per prey (effectively control compression = 10) ###\n",
        "        #sums = [sum([int(element) for element in ctrl])/len(ctrls[0]) for ctrl in ctrls] #allctrls\n",
        "        sums = [sum(sorted([int(element) for element in ctrl], reverse=True)[:10])/10 for ctrl in ctrls] #top 10 only\n",
        "        df['AvgCtrl'] = sums\n",
        "        df['CorrectedAvgSpec'] = df['AvgSpec'] - df['AvgCtrl']\n",
        "        df = df[df['BFDR'] <= 0.01]\n",
        "        df = df.pivot_table(index=['Bait'], columns=['PreyGene'], values=['CorrectedAvgSpec'])\n",
        "        df = df.fillna(0)\n",
        "        df = df.clip(lower=0)\n",
        "        df.columns = df.columns.droplevel()\n",
        "        scaler = MinMaxScaler()\n",
        "        df_norm = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\n",
        "        return df_norm\n",
        "\n",
        "    # Read and preprocess the data\n",
        "    df_original = pd.read_csv('7013_cleaned_v2.txt', sep='\\t')\n",
        "    df_original = norm_avg_ctrl(df_original)\n",
        "\n",
        "    # Perform NMF clustering\n",
        "    nmf = NMF(n_components=n_components, init='nndsvd', l1_ratio=1, random_state=46)\n",
        "    scores_original = nmf.fit_transform(df_original)\n",
        "    basis_original = nmf.components_.T\n",
        "\n",
        "    scores_df_original = pd.DataFrame(scores_original)\n",
        "    basis_df_original = pd.DataFrame(basis_original)\n",
        "\n",
        "    # Set the index to prey names\n",
        "    basis_df_original.index = df_original.columns\n",
        "    scores_df_original.index = df_original.index\n",
        "\n",
        "    # Find the maximum score for each component in the basis matrix\n",
        "    max_scores_per_column = basis_df_original.max()\n",
        "\n",
        "    # Apply the XX% threshold check for primary, secondary, and tertiary scores\n",
        "    primary_cluster = basis_df_original.idxmax(axis=1)\n",
        "    secondary_cluster = basis_df_original.apply(lambda row: row.nlargest(2).idxmin(), axis=1)\n",
        "    tertiary_cluster = basis_df_original.apply(lambda row: row.nlargest(3).idxmin(), axis=1)\n",
        "\n",
        "    # Update the basis_matrix_original with new columns\n",
        "    basis_matrix_original = basis_df_original.copy()\n",
        "\n",
        "    # Calculate primary, secondary, and tertiary scores\n",
        "    for idx, row in basis_df_original.iterrows():\n",
        "        sorted_scores = row.sort_values(ascending=False)\n",
        "\n",
        "        # Primary score and cluster\n",
        "        primary_score = sorted_scores.iloc[0]\n",
        "        primary_cluster_id = primary_cluster[idx]\n",
        "        basis_matrix_original.at[idx, 'primary score'] = primary_score\n",
        "        # Check if primary cluster meets the thresholds\n",
        "        if primary_score >= 0.05 * max_scores_per_column[primary_cluster_id]:\n",
        "            basis_matrix_original.at[idx, 'primary cluster'] = primary_cluster_id\n",
        "        else:\n",
        "            basis_matrix_original.at[idx, 'primary cluster'] = 'NA'\n",
        "\n",
        "        # Secondary score and cluster\n",
        "        secondary_cluster_id = secondary_cluster[idx]\n",
        "        if len(sorted_scores) > 1 and secondary_cluster_id != 'NA':\n",
        "            secondary_score = sorted_scores.iloc[1]\n",
        "            # Check if secondary cluster meets the thresholds\n",
        "            if secondary_score >= 0.70 * primary_score and secondary_score >= 0.05 * max_scores_per_column[secondary_cluster_id]:\n",
        "                basis_matrix_original.at[idx, 'secondary score'] = secondary_score\n",
        "                basis_matrix_original.at[idx, 'secondary cluster'] = secondary_cluster_id\n",
        "            else:\n",
        "                basis_matrix_original.at[idx, 'secondary score'] = secondary_score\n",
        "                basis_matrix_original.at[idx, 'secondary cluster'] = 'NA'\n",
        "        else:\n",
        "            basis_matrix_original.at[idx, 'secondary score'] = secondary_score\n",
        "            basis_matrix_original.at[idx, 'secondary cluster'] = 'NA'\n",
        "\n",
        "        # Tertiary score and cluster\n",
        "        tertiary_cluster_id = tertiary_cluster[idx]\n",
        "        if len(sorted_scores) > 2 and tertiary_cluster_id != 'NA':\n",
        "            tertiary_score = sorted_scores.iloc[2]\n",
        "            # Check if tertiary cluster meets the thresholds\n",
        "            if tertiary_score >= 0.70 * primary_score and tertiary_score >= 0.05 * max_scores_per_column[tertiary_cluster_id]:\n",
        "                basis_matrix_original.at[idx, 'tertiary score'] = tertiary_score\n",
        "                basis_matrix_original.at[idx, 'tertiary cluster'] = tertiary_cluster_id\n",
        "            else:\n",
        "                basis_matrix_original.at[idx, 'tertiary score'] = tertiary_score\n",
        "                basis_matrix_original.at[idx, 'tertiary cluster'] = 'NA'\n",
        "        else:\n",
        "            basis_matrix_original.at[idx, 'tertiary score'] = tertiary_score\n",
        "            basis_matrix_original.at[idx, 'tertiary cluster'] = 'NA'\n",
        "\n",
        "    # Create clustered_samples dictionary\n",
        "    clustered_samples = {i: [] for i in range(n_components)}\n",
        "\n",
        "    for column in clustered_samples.keys():\n",
        "        temp_df = basis_matrix_original[basis_matrix_original['primary cluster'] == column].copy()\n",
        "        temp_df['score'] = temp_df['primary score']\n",
        "        for cluster_type in ['secondary', 'tertiary']:\n",
        "            temp_cluster_column = f'{cluster_type} cluster'\n",
        "            temp_score_column = f'{cluster_type} score'\n",
        "            temp_df_secondary = basis_matrix_original[basis_matrix_original[temp_cluster_column] == column].copy()\n",
        "            temp_df_secondary['score'] = temp_df_secondary[temp_score_column]\n",
        "            temp_df = pd.concat([temp_df, temp_df_secondary])\n",
        "        temp_df = temp_df.sort_values(by='score', ascending=False)\n",
        "        clustered_samples[column] = temp_df.index.tolist()\n",
        "\n",
        "    ### write bait and prey matrices to .csv\n",
        "    basis_matrix_original.to_csv(f'7013_cleaned_v2_NMF_{n_components}_top10-07-005_prey_matrix.csv')\n",
        "    scores_df_original.to_csv(f'7013_cleaned_v2_NMF_{n_components}_top10-07-005_bait_matrix.csv')\n",
        "\n",
        "    # Initialize g:Profiler\n",
        "    gp = GProfiler(return_dataframe=True)\n",
        "\n",
        "    # Create a new Excel writer for each k value\n",
        "    with pd.ExcelWriter(f'7013_cleaned_v2_NMF_{n_components}_top10-07-005_clusters_enrichment.xlsx') as writer:\n",
        "        for label, preys in clustered_samples.items():\n",
        "\n",
        "            # Query g:Profiler for GO terms\n",
        "            results = gp.profile(organism='gp__2aO3_xfUn_hEA', query=preys, sources=[])\n",
        "            # print(results)\n",
        "\n",
        "            # Write to a separate sheet in the Excel file\n",
        "            results.to_excel(writer, sheet_name=f'Cluster_{label}', index=False)\n",
        "\n",
        "        # Find the maximum length among the lists in clustered_samples\n",
        "    max_length = max(len(indices) for indices in clustered_samples.values())\n",
        "\n",
        "    # Pad each list in clustered_samples to match the maximum length\n",
        "    for key in clustered_samples:\n",
        "        clustered_samples[key].extend([None] * (max_length - len(clustered_samples[key])))\n",
        "\n",
        "    # Convert clustered_samples to a DataFrame and write to a CSV file\n",
        "    clustered_samples_df = pd.DataFrame.from_dict(clustered_samples, orient='index').transpose()\n",
        "    clustered_samples_df.to_csv(f'7013_cleaned_v2_NMF_{n_components}_top10-07-005_clusters.csv', index=False)\n"
      ],
      "metadata": {
        "id": "h7nTagvaDC0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the desired number of components\n",
        "# k = 19 is what was used for the final published results\n",
        "nmf_cluster_analysis_filtering(19)"
      ],
      "metadata": {
        "id": "9HZWMk4GJRp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function with the desired number of components\n",
        "# for Dyakov et al. 2014 we tested k = 10 to 24 and conducted precision-recall analysis to determine that k = 19 should be used\n",
        "# see Methods and associated R code in the repo for this manuscript\n",
        "nmf_cluster_analysis_filtering(10)\n",
        "nmf_cluster_analysis_filtering(11)\n",
        "nmf_cluster_analysis_filtering(12)\n",
        "nmf_cluster_analysis_filtering(13)\n",
        "nmf_cluster_analysis_filtering(14)\n",
        "nmf_cluster_analysis_filtering(15)\n",
        "nmf_cluster_analysis_filtering(16)\n",
        "nmf_cluster_analysis_filtering(17)\n",
        "nmf_cluster_analysis_filtering(18)\n",
        "nmf_cluster_analysis_filtering(19)\n",
        "nmf_cluster_analysis_filtering(20)\n",
        "nmf_cluster_analysis_filtering(21)\n",
        "nmf_cluster_analysis_filtering(22)\n",
        "nmf_cluster_analysis_filtering(23)\n",
        "nmf_cluster_analysis_filtering(24)"
      ],
      "metadata": {
        "id": "uSDpul5UCU6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming NMF prey matrix into prohits-viz compatible file (Fig. 3a, b & Extended Data Fig. 6)\n",
        "\n",
        "This code is to transform data for creation of top N preys per rank/cluster, which can be made into a heatmap in prohits-viz, where clustering occurs."
      ],
      "metadata": {
        "id": "R_mRsJQMliK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from a CSV file\n",
        "file_path = '7013_cleaned_v2_NMF_19_top10-07-005_prey_matrix-UpdatedHeaders.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Specify the column to sort by and the number of top rows to select\n",
        "sort_column_index = 15  # column in excel numbering (not python) corresponding to cluster to sort by, e.g. use 1 for PS (cluster 0) and 13 for NS (cluster 12)\n",
        "top_n = 50  # Number of top entries based on the sort column\n",
        "\n",
        "# Sort the data in descending order by the specified column and select the top N entries\n",
        "sorted_data = data.sort_values(by=data.columns[sort_column_index], ascending=False).head(top_n)\n",
        "\n",
        "# Prepare the data for melting (transforming)\n",
        "output_columns = ['PreyGene'] + list(data.columns[1:20])  # Columns from 2 to 20 are cluster scores\n",
        "output_data = sorted_data[output_columns]\n",
        "\n",
        "# Melt the data into a long format\n",
        "melted_data = output_data.melt(id_vars=['PreyGene'], var_name='Cluster', value_name='NMF Score')\n",
        "\n",
        "# Replace column header numbers with 'Cluster x'\n",
        "melted_data['Cluster'] = melted_data['Cluster']#.apply(lambda x: 'Cluster ' + x)\n",
        "\n",
        "# Dynamically name the output file based on the selected column and top N\n",
        "output_file_name = f\"output_sorted_by_cluster_{sort_column_index-1}_top_{top_n}.txt\"\n",
        "output_file_path = output_file_name  # Adjust this path as necessary\n",
        "\n",
        "# Save the transformed data to a tab-delimited txt file\n",
        "melted_data.to_csv(output_file_path, sep='\\t', index=False)\n",
        "\n",
        "print(f\"Output saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5wFLFWulnUk",
        "outputId": "db814ca9-bf8c-4dfd-9729-6a21a848de34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output saved to output_sorted_by_cluster_14_top_50.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prey-prey Pearson correlations based on NMF scores (Fig. 2a)"
      ],
      "metadata": {
        "id": "k9jB8B2rU98j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code used to compute prey-prey Pearson correlations to be added here"
      ],
      "metadata": {
        "id": "JyBKLnXJ6Q22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Filtering prey-prey correlations to produce input files for cytoscape (For Fig. 2a)\n",
        "\n"
      ],
      "metadata": {
        "id": "wlANdhkO6Xdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data from the input CSV file\n",
        "df = pd.read_csv('preys_NMF_correlations_all.csv')\n",
        "\n",
        "# Apply the filter to retain rows where 'Correlation' is greater than or equal to 0.5 or other value\n",
        "filtered_df = df[df['Correlation'] >= 0.6]\n",
        "\n",
        "# Write the filtered data to a new CSV file\n",
        "filtered_df.to_csv('preys_NMF_correlations_06.csv', index=False)\n"
      ],
      "metadata": {
        "id": "-TCPabGpU8we"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}